---
---



@inproceedings{
spanishmi,
title={Examining Spanish Counseling with {MIDAS}: a Motivational Interviewing Dataset in Spanish},
author={Aylin Ece Gunal and Bowen Yi and John D. Piette and Rada Mihalcea and Veronica Perez-Rosas},
booktitle={The 2025 Annual Conference of the Nations of the Americas Chapter of the ACL},
year={2025},
url={https://openreview.net/forum?id=tReVq0bFUX}
}

@inproceedings{Research-jam-2024,
  title={Causally Modeling the Linguistic and Social Factors that Predict Email ResponseCausally Modeling the Linguistic and Social Factors that Predict Email Response},
  author={Yinuo Xu* and Hong Chen* and Sushrita Rakshit* and Aparna Ananthasubramaniam* and Omkar Yadav* and Mingqian Zheng* and Michael Jiang* and Lechen Zhang* and Bowen Yi* and Kenan Alkiek* and Abraham Israeli* and Bangzhao Shu* and Hua Shen* and Jiaxin Pei* and Haotian Zhang* and Miriam Schirmer* and David Jurgens},
  booktitle={The 2025 Annual Conference of the Nations of the Americas Chapter of the ACL},
year={2025},
url={https://openreview.net/forum?id=2JwDE5xWxk}
  abstract={Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.},
}

@misc{yi2025unveilingbehavioraldifferencesbilingual,
      title={Unveiling Behavioral Differences in Bilingual Information Operations: A Network-Based Approach}, 
      author={Bowen Yi},
      note={In Submission to IC2S2, Preprint on arXiv},
      year={2025},
      eprint={2501.09027},
      archivePrefix={arXiv},
      primaryClass={cs.SI},
      url={https://arxiv.org/abs/2501.09027}, 
      abstract={Twitter has become a pivotal platform for conducting information operations (IOs), particularly during high-stakes political events. In this study, we analyze over a million tweets about the 2024 U.S. presidential election to explore an under-studied area: the behavioral differences of IO drivers from English- and Spanish-speaking communities. Using similarity graphs constructed from behavioral patterns, we identify IO drivers in both languages and evaluate the clustering quality of these graphs in an unsupervised setting. Our analysis demonstrates how different network dismantling strategies, such as node pruning and edge filtering, can impact clustering quality and the identification of coordinated IO drivers. We also reveal significant differences in the topics and political indicators between English and Spanish IO drivers. Additionally, we investigate bilingual users who post in both languages, systematically uncovering their distinct roles and behaviors compared to monolingual users. These findings underscore the importance of robust, culturally and linguistically adaptable IO detection methods to mitigate the risks of influence campaigns on social media. Our code and data are available on GitHub: this https URL. }
}

@unpublished{medicalintern,
  title={Uncovering the Impact of Intervention Messages on Diverse Population Groups},
  author={Bowen Yi and Rada Mihalcea and Fang Yu and Elena Frank and Joan Zhao and Srijan Sen and Maggie Makar},
  note={Working Paper},
  year={2024}, 
  abstract={Medical interns usually face mental health challenges due to the demanding nature of their working environments. In this context, intervention messages are delivered to their mobile devices to support them in maintaining healthy behavior. This study investigates the effectiveness of various intervention messages by analyzing their conditional average causal effects on health outcomes such as steps, mood, and sleep. We identify linguistic features, including LIWC, and factors encoded by medical experts, such as cognitive strategies, that contribute to the efficacy of these interventions. To handle the high dimensionality of our treatment variables, we develop methods that balance the tradeoff between overlapping and positivity assumptions of causal inference. In addition, we introduce a decision tree-based approach to identify subgroups that respond differently to interventions. It shows higher stability than previous approaches against small data variations. Our research, validated on a 4-year dataset involving over 6,000 medical interns from 250 U.S. medical programs, offers valuable insights for designing more effective, subgroup-aware intervention strategies in healthcare.},
}

@misc{Research-jam-summer-2024,
  title={Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue},
  author={Bowen Yi* and Johnathan Ivey* and Shivani Kumar* and Jiayu Liu* and Hua Shen* and Sushrita Rakshit* and Rohan Raju* and Haotian Zhang* and Aparna Ananthasubramaniam* and Junghwan Kim* and Dustin Wright* and Abraham Israeli* and Anders Giovanni MÃ¸ller* and Lechen Zhang* and David Jurgens},
  note={In Submission to ACL 2025. * denotes equal contribution with randomized order.},
  year={2024}, 
  eprint={2404.08760},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url = "https://arxiv.org/abs/2409.08330v1",
  pdf = "https://arxiv.org/pdf/2409.08330v1",
  abstract={Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.},
}


@misc{Siyang-2024,
  title={The Generation Gap: Exploring Age Bias Underlying in the Value
Systems of Large Language Models},
  author={Liu, Siyang and Maturi, Trish and Yi, Bowen and Shen, Siqi and Mihalcea, Rada},
  note={EMNLP 2024},
  year={2024}, 
  eprint={2404.08760},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url = "https://arxiv.org/abs/2404.08760",
  pdf = "https://arxiv.org/pdf/2404.08760.pdf",
  abstract={In this paper, we explore the alignment of values in Large Language Models (LLMs) 
  with specific age groups, leveraging data from the World Value Survey across thirteen 
  categories. Through a diverse set of prompts tailored to ensure response robustness, 
  we find a general inclination of LLM values towards younger demographics, 
  especially in the US. Additionally, we explore the impact of incorporating 
  age identity information in prompts and observe challenges in mitigating value 
  discrepancies with different age cohorts. Our findings highlight the age bias in 
  LLMs and provide insights for future work. Materials for our analysis will be 
  available via anonymous.github.com},
}
