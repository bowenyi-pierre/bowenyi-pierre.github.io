---
---
@misc{Research-jam-summer-2024,
  title={Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue},
  author={Bowen Yi* and Johnathan Ivey* and Shivani Kumar* and Jiayu Liu* and Hua Shen* and Sushrita Rakshit* and Rohan Raju* and Haotian Zhang* and Aparna Ananthasubramaniam* and Junghwan Kim* and Dustin Wright* and Abraham Israeli* and Anders Giovanni MÃ¸ller* and Lechen Zhang* and David Jurgens},
  note={In Submission to NAACL 2025. * denotes equal contribution.},
  year={2024}, 
  eprint={2404.08760},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url = "https://arxiv.org/abs/2409.08330v1",
  pdf = "https://arxiv.org/pdf/2409.08330v1",
  abstract={Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.},
}


@unpublished{Research-jam-winter-2024,
  title={Please Reply: Causally Modeling the Linguistic and Social Factors that Predict Email Response},
  author={Bowen Yi* and Yinuo Xu* and Hong Chen* and Sushrita Rakshit* and Aparna Ananthasubramaniam* and Omkar Yadav* and Mingqian Zheng* and Michael Jiang* and Lechen Zhang* and Kenan Alkiek* and Abraham Israeli* and Bangzhao Shu* and Hua Shen* and Jiaxin Pei* and Haotian Zhang* and Miriam Schirmer* and David Jurgens},
  note={* denotes equal contribution.},
  year={2024},
  abstract={Email serves as a vital conduit for human communication across businesses, organizations, and broader societal contexts. In this study, we aim to model the intents, expectations, and responsiveness in email exchanges. To this end, we create \dataset, a new dataset containing 1800 emails annotated with nuanced types of intents and expectations. We benchmark models ranging from feature-based logistic regression to zero-shot prompting of large language models. Leveraging the predictive model for intent, expectations, and 14 other categories of pragmatic features, we analyze 11.3M emails from GMANE to study the conversational dynamics in email exchanges. Through our causal analysis, we find that the email response rates are influenced by social status, argumentation, and in certain limited contexts, the strength of social connection. Our code and data are available upon acceptance.},
}


@misc{Siyang-2024,
  title={The Generation Gap: Exploring Age Bias Underlying in the Value
Systems of Large Language Models},
  author={Liu, Siyang and Maturi, Trish and Yi, Bowen and Shen, Siqi and Mihalcea, Rada},
  note={EMNLP 2024 (accepted)},
  year={2024}, 
  eprint={2404.08760},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url = "https://arxiv.org/abs/2404.08760",
  pdf = "https://arxiv.org/pdf/2404.08760.pdf",
  abstract={In this paper, we explore the alignment of values in Large Language Models (LLMs) 
  with specific age groups, leveraging data from the World Value Survey across thirteen 
  categories. Through a diverse set of prompts tailored to ensure response robustness, 
  we find a general inclination of LLM values towards younger demographics, 
  especially in the US. Additionally, we explore the impact of incorporating 
  age identity information in prompts and observe challenges in mitigating value 
  discrepancies with different age cohorts. Our findings highlight the age bias in 
  LLMs and provide insights for future work. Materials for our analysis will be 
  available via anonymous.github.com},
}
