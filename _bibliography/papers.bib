---
---
@unpublished{medicalintern,
  title={Uncovering the Impact of Intervention Messages on Diverse Population Groups},
  author={Bowen Yi* and Rada Mihalcea and Fang Yu and Elena Frank and Joan Zhao and Srijan Sen and Maggie Makar},
  note={Working Paper},
  year={2024}, 
  abstract={Medical interns usually face mental health challenges due to the demanding nature of their working environments. In this context, intervention messages are delivered to their mobile devices to support them in maintaining healthy behavior. This study investigates the effectiveness of various intervention messages by analyzing their conditional average causal effects on health outcomes such as steps, mood, and sleep. We identify linguistic features, including LIWC, and factors encoded by medical experts, such as cognitive strategies, that contribute to the efficacy of these interventions. To handle the high dimensionality of our treatment variables, we develop methods that balance the tradeoff between overlapping and positivity assumptions of causal inference. In addition, we introduce a decision tree-based approach to identify subgroups that respond differently to interventions. It shows higher stability than previous approaches against small data variations. Our research, validated on a 4-year dataset involving over 6,000 medical interns from 250 U.S. medical programs, offers valuable insights for designing more effective, subgroup-aware intervention strategies in healthcare.},
}

@misc{Research-jam-summer-2024,
  title={Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue},
  author={Bowen Yi* and Johnathan Ivey* and Shivani Kumar* and Jiayu Liu* and Hua Shen* and Sushrita Rakshit* and Rohan Raju* and Haotian Zhang* and Aparna Ananthasubramaniam* and Junghwan Kim* and Dustin Wright* and Abraham Israeli* and Anders Giovanni MÃ¸ller* and Lechen Zhang* and David Jurgens},
  note={In Submission to NAACL 2025. * denotes equal contribution.},
  year={2024}, 
  eprint={2404.08760},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url = "https://arxiv.org/abs/2409.08330v1",
  pdf = "https://arxiv.org/pdf/2409.08330v1",
  abstract={Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.},
}


@misc{Siyang-2024,
  title={The Generation Gap: Exploring Age Bias Underlying in the Value
Systems of Large Language Models},
  author={Liu, Siyang and Maturi, Trish and Yi, Bowen and Shen, Siqi and Mihalcea, Rada},
  note={EMNLP 2024 (accepted)},
  year={2024}, 
  eprint={2404.08760},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url = "https://arxiv.org/abs/2404.08760",
  pdf = "https://arxiv.org/pdf/2404.08760.pdf",
  abstract={In this paper, we explore the alignment of values in Large Language Models (LLMs) 
  with specific age groups, leveraging data from the World Value Survey across thirteen 
  categories. Through a diverse set of prompts tailored to ensure response robustness, 
  we find a general inclination of LLM values towards younger demographics, 
  especially in the US. Additionally, we explore the impact of incorporating 
  age identity information in prompts and observe challenges in mitigating value 
  discrepancies with different age cohorts. Our findings highlight the age bias in 
  LLMs and provide insights for future work. Materials for our analysis will be 
  available via anonymous.github.com},
}
